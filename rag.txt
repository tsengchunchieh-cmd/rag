import pandas as pd
import numpy as np
import json
import math
import random
from typing import List, Dict, Any, Tuple

# è­¦å‘Šï¼šä»¥ä¸‹ç¨‹å¼ç¢¼å¡Šå‡è¨­åœ¨ Google Colab æˆ– Jupyter ç’°å¢ƒä¸­é‹è¡Œæ™‚ï¼Œ
# æª”æ¡ˆä¸Šå‚³ (files.upload) å’Œä¸‹è¼‰ (files.download) å‡½æ•¸æ˜¯å¯ç”¨çš„ã€‚
# åœ¨æ¨™æº– Python ç’°å¢ƒä¸­ï¼Œæ‚¨éœ€è¦è¨»é‡‹æˆ–æ›¿æ›é€™äº›å‡½æ•¸ã€‚
try:
    from google.colab import files
    # æ¨¡æ“¬ Colab ç’°å¢ƒçš„å®‰è£
    # !pip -q install pandas numpy scikit-learn
    print("Google Colab environment detected. Using mock file operations.")
except ImportError:
    # æ¨™æº– Python ç’°å¢ƒä¸‹ï¼Œå®šç¾©è™›æ“¬å‡½æ•¸é¿å…éŒ¯èª¤
    class MockFiles:
        def upload(self):
            print("--- MOCK: files.upload() - è«‹åœ¨å¯¦éš›ç’°å¢ƒä¸­ä¸Šå‚³ CSV æª”æ¡ˆ ---")
            return {}
        def download(self, filename):
            print(f"--- MOCK: files.download('{filename}') - æª”æ¡ˆå·²ç”Ÿæˆï¼Œä½†æœªå¯¦éš›ä¸‹è¼‰ ---")
    files = MockFiles()
    print("Standard Python environment detected. Using mock file operations.")

# --- 1. æ ¸å¿ƒè¼”åŠ©å‡½å¼å®šç¾© ---

def normalize_ability_name(name: str) -> str:
    """çµ±ä¸€èƒ½åŠ›æŒ‡æ¨™åç¨±æ ¼å¼ï¼šå»é™¤å‰å¾Œç©ºç™½ã€çµ±ä¸€å…¨å½¢èˆ‡åŠå½¢ç¬¦è™Ÿã€‚"""
    if not isinstance(name, str):
        return ""
    return (
        name.strip()
        .replace("ã€€", " ")
        .replace("ï¼", "/")
        .replace("Â  ", " ")
    )

def describe_abilities(abilities: List[str], desc_map: Dict[str, str]) -> str:
    """å°‡èƒ½åŠ›æŒ‡æ¨™è½‰ç‚ºå…·ä¸­æ–‡èªªæ˜çš„å¯è®€æ–‡å­—ã€‚"""
    return "ã€".join(
        f"{a}ï¼ˆ{desc_map.get(a, 'èƒ½åŠ›æè¿°å¾…è£œ')}ï¼‰"
        for a in abilities
    )

def build_rule_base(df: pd.DataFrame) -> List[Dict[str, Any]]:
    """å°‡é—œè¯è¦å‰‡ DataFrame è½‰ç‚ºçŸ¥è­˜è¦å‰‡åº«çµæ§‹ã€‚"""
    rule_base = []
    for _, row in df.iterrows():
        rule_base.append({
            "antecedent": [a.strip() for a in row["å‰ä»¶"].split(",")],
            "consequent": [c.strip() for c in row["å¾Œä»¶"].split(",")],
            "support": float(row["support"]),
            "confidence": float(row["confidence"]),
            "lift": float(row["lift"]),
            "explain": row["explain"]
        })
    return rule_base

def score_rule(student_abilities: List[str], rule: Dict[str, Any], w_conf: float = 0.5, w_lift: float = 0.5) -> float:
    """è¨ˆç®—å–®æ¢è¦å‰‡èˆ‡å­¸ç”ŸéŒ¯èª¤èƒ½åŠ›ä¹‹ç›¸é—œæ€§åˆ†æ•¸ã€‚"""
    student_set = set(a.strip() for a in student_abilities)
    antecedent_set = set(rule["antecedent"])

    # å‰ä»¶èƒ½åŠ›é‡ç–Šæ¯”ä¾‹
    overlap_ratio = len(student_set & antecedent_set) / (len(antecedent_set) + 1e-9)

    # ç¶œåˆä¿¡è³´åº¦èˆ‡æå‡åº¦ (lift ç¸®æ”¾é¿å…ä¸»å°)
    quality_score = (
        w_conf * rule["confidence"] +
        w_lift * min(rule["lift"] / 2.0, 1.0)
    )
    return overlap_ratio * quality_score

def retrieve_rules(student_abilities: List[str], rule_base: List[Dict[str, Any]], top_k: int = 5) -> List[Tuple[float, Dict[str, Any]]]:
    """æ ¹æ“šå­¸ç”ŸéŒ¯èª¤èƒ½åŠ›ï¼Œæª¢ç´¢æœ€ç›¸é—œä¹‹ Top-Kæ¢è¦å‰‡ã€‚"""
    scored_rules = [
        (score_rule(student_abilities, rule), rule)
        for rule in rule_base
    ]
    scored_rules = sorted(scored_rules, key=lambda x: x[0], reverse=True)
    
    # åƒ…å›å‚³åˆ†æ•¸å¤§æ–¼ 0 çš„è¦å‰‡
    return [(score, rule) for score, rule in scored_rules[:top_k] if score > 0]

def build_target_abilities(student_abilities: List[str], retrieved_rules: List[Tuple[float, Dict[str, Any]]]) -> List[str]:
    """èšåˆå­¸ç”Ÿå·²éŒ¯èƒ½åŠ›èˆ‡è¦å‰‡é—œè¯çš„èƒ½åŠ›ï¼Œç”Ÿæˆæœ‰åºçš„è£œå¼·ç›®æ¨™æ¸…å–®ã€‚"""
    A = set([a.strip() for a in student_abilities])
    cand = set()
    for score, r in retrieved_rules:
        cand.update(r["antecedent"])
        cand.update(r["consequent"])
        
    target = list(A | cand)
    # æ’åºç­–ç•¥ï¼šå„ªå…ˆå°‡å­¸ç”Ÿå·²éŒ¯çš„æ”¾å‰é¢ (x not in A ç‚º False/0)ï¼Œå…¶æ¬¡æŒ‰å­—æ¯æ’åº
    target = sorted(set(target), key=lambda x: (x not in A, x))
    return target

# --- 2. é¡Œç›®ç”Ÿæˆå™¨ï¼ˆStep 6 é‚è¼¯ï¼‰ ---

def gen_item(ability: str, q: str, opts: List[str], ans_idx: int, rationale: str):
    """æ¨™æº–åŒ–é¡Œç›®ç”Ÿæˆè¼¸å‡ºçµæ§‹"""
    return {
        "ability": ability,
        "question": q,
        "options": opts,
        "answer": opts[ans_idx],
        "rationale": rationale
    }

def gen_word_usage():
    q = "ä¾èªå¢ƒï¼Œå¡«å…¥æœ€æ°ç•¶çš„è©èªï¼šã€é¢å°æŒ‘æˆ°ï¼Œä»–ä¾ç„¶ï¼ˆï¼‰ä¸äº‚ã€‚ã€"
    opts = ["é©šæ…Œ", "é®å®š", "å‹•æ–", "å¤±æª"]
    return gen_item("å­—è©_æ‡‰ç”¨", q, opts, 1, "åŸ¹é¤Šèªå¢ƒä¸‹çš„è©èªé¸æ“‡èƒ½åŠ›")

def gen_sentence_reading():
    q = "åˆ¤æ–·èªæ°£ï¼šã€ä½ é€™æ¬¡å¯çœŸæ˜¯â€œå²å®³â€å‘¢ã€‚ã€æ­¤è™•â€œå²å®³â€æœ€å¯èƒ½è¡¨é”ï¼Ÿ"
    opts = ["ç¨±è®š", "é©šè¨", "åè«·", "æ†¤æ€’"]
    return gen_item("å¥æ®µ_é–±è®€", q, opts, 2, "å¼·åŒ–å¥æ®µç†è§£èˆ‡èªç”¨åˆ¤æ–·")

def gen_paragraph_reading():
    q = "ä»¥ä¸‹å“ªä¸€å¥æœ€èƒ½ä½œç‚ºé€™æ®µæ–‡å­—çš„ã€ä¸»æ—¨å¥ã€ï¼Ÿï¼ˆä¸»é¡Œï¼šå›æ”¶åˆ†é¡çš„é‡è¦æ€§ï¼‰"
    opts = ["å›æ”¶å¯ä»¥è³ºéŒ¢ã€‚", "å›æ”¶åˆ†é¡èƒ½é™ä½è³‡æºæµªè²»ä¸¦ä¿è­·ç’°å¢ƒã€‚", "æˆ‘å®¶é™„è¿‘æœ‰å›æ”¶ç«™ã€‚", "å¡‘è† æ¯”ç´™æ›´ä¸ç’°ä¿ã€‚"]
    return gen_item("ç¯‡ç« _é–±è®€", q, opts, 1, "åŸ¹é¤Šç¯‡ç« çµæ§‹èˆ‡æ¨è«–èƒ½åŠ›")

GENERATOR_MAP = {
    "å­—è©_æ‡‰ç”¨": gen_word_usage,
    "å¥æ®µ_é–±è®€": gen_sentence_reading,
    "ç¯‡ç« _é–±è®€": gen_paragraph_reading,
    "å¥æ®µ_æœ—è®€èˆ‡é–±è®€": gen_sentence_reading,
    "ç¯‡ç« _æœ—è®€èˆ‡é–±è®€": gen_paragraph_reading,
    # ç°¡åŒ–å…¶ä»–èƒ½åŠ›ï¼Œä»¥ gen_sentence_reading ä½œç‚ºé è¨­
}

def generate_items_for_ability(ability: str, n: int = 2) -> List[Dict[str, Any]]:
    """æ ¹æ“šæŒ‡å®šèƒ½åŠ›ï¼Œç”Ÿæˆ n é¡Œè£œæ•‘æ•™å­¸ç·´ç¿’"""
    generator_fn = GENERATOR_MAP.get(ability, gen_sentence_reading)
    # ç‚ºäº†æ¨¡æ“¬å¤šé¡Œï¼Œæˆ‘å€‘ç°¡å–®é‡è¤‡èª¿ç”¨ç”Ÿæˆå™¨
    return [generator_fn() for _ in range(n)]

# --- 3. Markdown å ±å‘Šç”Ÿæˆå‡½å¼ï¼ˆStep 8 é‚è¼¯ï¼‰ ---

def render_markdown_packet(student_abilities: List[str], retrieved_rules: List[Tuple[float, Dict[str, Any]]], topN_df: pd.DataFrame, n_per_ability: int = 2) -> str:
    """å°‡è¨ºæ–·çµæœæ•´åˆä¸¦è¼¸å‡ºç‚ºæ ¼å¼åŒ–çš„ Markdown æ–‡ä»¶ã€‚"""
    md = []
    
    md.append("# ğŸ“ å€‹äººåŒ–è£œæ•‘å­¸ç¿’åŒ…èˆ‡è¨ºæ–·å ±å‘Š\n")
    
    # I. è¨ºæ–·æ‘˜è¦
    md.append("## ğŸ’¡ ä¸€ã€è¨ºæ–·æ‘˜è¦ï¼šå­¸ç”Ÿç•¶å‰å¼±é»åˆ†æ\n")
    md.append("* **å­¸ç”Ÿå·²éŒ¯èƒ½åŠ›æ¨™ç±¤ (A)**ï¼š\n")
    md.append("    * " + "ã€".join(student_abilities) + "\n")
    
    # II. è¦å‰‡åˆ†æ
    md.append("\n---\n")
    md.append("## ğŸ”— äºŒã€é—œè¯è¦å‰‡åˆ†æ (RAG æª¢ç´¢çµæœ)\n")
    if retrieved_rules:
        for score, r in retrieved_rules:
            ante = "ã€".join(r["antecedent"])
            cons = "ã€".join(r["consequent"])
            md.append(f"* **è¦å‰‡**ï¼šè‹¥åœ¨ {ante} éŒ¯é¡Œï¼Œå‰‡åœ¨ {cons} ä¹ŸæœƒéŒ¯çš„æ©Ÿç‡åé«˜ã€‚ï¼ˆScore={score:.3f}ï¼‰\n")
    else:
        md.append("* æœªæª¢ç´¢åˆ°èˆ‡å­¸ç”ŸéŒ¯èª¤èƒ½åŠ›é«˜åº¦ç›¸é—œçš„è¦å‰‡ã€‚\n")

    # III. è£œæ•‘æ ¸å¿ƒ
    md.append("\n---\n")
    md.append("## ğŸ¯ ä¸‰ã€è£œæ•‘æ ¸å¿ƒï¼šå‡ºé¡Œç„¦é»èƒ½åŠ› (Target Abilities)\n")
    focus = build_target_abilities(student_abilities, retrieved_rules)
    md.append("* **ç„¦é»èƒ½åŠ›æ¸…å–®**ï¼š\n")
    md.append("    * " + "ã€".join(focus) + "\n")
    
    # IV. ç·´ç¿’é¡Œ
    md.append("\n---\n")
    md.append("## ğŸ“š å››ã€å€‹äººåŒ–ç·´ç¿’é¡Œèˆ‡ç­”æ¡ˆè§£æ (N={})\n".format(n_per_ability))
    for ab in focus:
        md.append(f"### {ab}\n")
        items = generate_items_for_ability(ab, n=n_per_ability)
        for idx, it in enumerate(items, 1):
            opts = " / ".join(it["options"])
            md.append(f"**Q{idx}.** {it['question']}\n")
            md.append(f"* é¸é …ï¼š{opts}\n* **åƒè€ƒç­”æ¡ˆ**ï¼š{it['answer']}\n* å‡ºé¡Œæ„åœ–ï¼š{it['rationale']}\n")
        md.append("\n")

    # V. æ•™å­¸å»ºè­°
    md.append("\n---\n")
    md.append("## ğŸ§‘â€ğŸ« äº”ã€æ•™å­¸èˆ‡è¼”å°å»ºè­° (åŸºæ–¼è¦å‰‡é—œè¯)\n")
    md.append("- **åŸºç¤å…ˆè¡Œ**ï¼šæ ¹æ“šè¦å‰‡ (å­—è©_æ‡‰ç”¨ â†’ å¥æ®µ_é–±è®€) é¡¯ç¤ºï¼Œè«‹æ–¼æ®µè½é–±è®€å‰å…ˆè£œå¼·è©èªæ­é…èˆ‡èªå¢ƒè¾¨æã€‚\n")
    md.append("- **éæ¸¡èˆ‡éŠœæ¥**ï¼šå…ˆä»¥ã€Œå¥æ®µ_é–±è®€ã€å¸¶å…¥é€£æ¥è©ã€èªæ°£åˆ¤æ–·ï¼Œå†éæ¸¡è‡³ã€Œç¯‡ç« _é–±è®€ã€é€²è¡Œä¸»æ—¨èˆ‡çµæ§‹æ¨è«–ã€‚\n")
    
    # VI. å®è§€æ•¸æ“šåƒè€ƒ
    md.append("\n---\n")
    md.append("## ğŸ“Š å…­ã€å®è§€æ•¸æ“šåƒè€ƒï¼šèƒ½åŠ›éŒ¯èª¤ç‡ Top 10\n")
    # ä½¿ç”¨ to_markdown è¼¸å‡ºè¡¨æ ¼
    md.append(topN_df.to_markdown(index=False))
    
    return "\n".join(md)


# =================================================================
# ä¸»åŸ·è¡Œæµç¨‹
# =================================================================

print("\n--- [å°ˆé¡Œåˆå§‹åŒ–é–‹å§‹] ---\n")

# --- A. æ¨¡æ“¬è³‡æ–™ä¸Šå‚³èˆ‡è®€å– (Step 2, 3) ---

# 1. æ¨¡æ“¬ ability_error_rate.csv å…§å®¹
ability_df_mock = pd.DataFrame({
    "èƒ½åŠ›æŒ‡æ¨™_å–®é¡Œ": ['å¥æ®µ_é–±è®€', 'ç¯‡ç« _é–±è®€', 'å­—è©_æ‡‰ç”¨', 'ç¯‡ç« _çµæ§‹', 'å­—è©_èªè­˜'],
    "éŒ¯èª¤ç‡": [0.65, 0.58, 0.61, 0.45, 0.50]
})

# 2. æ¨¡æ“¬ frequent_wrong_sets.csv å…§å®¹ (åƒ…ç”¨æ–¼æ¨¡æ“¬æµç¨‹ï¼Œä¸ç›´æ¥å½±éŸ¿æœ€çµ‚å ±å‘Š)
freq_df_mock = pd.DataFrame({
    "é …ç›®": ['å¥æ®µ_é–±è®€,ç¯‡ç« _é–±è®€', 'å­—è©_æ‡‰ç”¨,å¥æ®µ_é–±è®€'],
    "support": [0.15, 0.12]
})

# 3. æ¨¡æ“¬ assoc_rules.csv å…§å®¹ (æœ€é‡è¦çš„è¦å‰‡æ•¸æ“š)
rules_df_mock = pd.DataFrame({
    "å‰ä»¶": ['å¥æ®µ_é–±è®€', 'å­—è©_æ‡‰ç”¨', 'ç¯‡ç« _çµæ§‹', 'å­—è©_èªè­˜'],
    "å¾Œä»¶": ['ç¯‡ç« _é–±è®€', 'å¥æ®µ_é–±è®€', 'ç¯‡ç« _é–±è®€', 'å­—è©_æ‡‰ç”¨'],
    "support": [0.15, 0.12, 0.10, 0.06],
    "confidence": [0.85, 0.72, 0.68, 0.60],
    "lift": [1.46, 1.25, 1.20, 1.15]
})

# åŸ·è¡Œ Step 3-3, 3-4, 3-5 çš„æ­£è¦åŒ– (åƒ…å°è¦å‰‡æ•¸æ“šåšé—œéµæ­£è¦åŒ–)
rules_df_mock["å‰ä»¶"] = rules_df_mock["å‰ä»¶"].apply(normalize_ability_name)
rules_df_mock["å¾Œä»¶"] = rules_df_mock["å¾Œä»¶"].apply(normalize_ability_name)


# --- B. å»ºç«‹çŸ¥è­˜è¦å‰‡åº« (Step 4) ---

# ç¯©é¸é–€æª»è¨­å®š
rule_thresholds = {
    "min_support": 0.05,
    "min_confidence": 0.60,
    "min_lift": 1.10
}

# ç¯©é¸é«˜å“è³ªé—œè¯è¦å‰‡ (Step 4-3)
filtered_rules_df = rules_df_mock[
    (rules_df_mock["support"] >= rule_thresholds["min_support"]) &
    (rules_df_mock["confidence"] >= rule_thresholds["min_confidence"]) &
    (rules_df_mock["lift"] >= rule_thresholds["min_lift"])
].copy()

# å»ºç«‹è¦å‰‡æ–‡å­—è§£é‡‹ (Step 4-4)
def build_rule_explanation_custom(row):
    return (
        f"è‹¥å­¸ç”Ÿåœ¨ {row['å‰ä»¶']} å‡ºç¾éŒ¯èª¤ï¼Œ"
        f"å‰‡åœ¨ {row['å¾Œä»¶']} ä¹Ÿæœƒå‡ºéŒ¯çš„æ©Ÿç‡åé«˜"
        f"ï¼ˆconfidence={row['confidence']:.2f}, "
        f"lift={row['lift']:.2f}ï¼‰ã€‚"
    )
filtered_rules_df["explain"] = filtered_rules_df.apply(build_rule_explanation_custom, axis=1)

# å»ºæ§‹çŸ¥è­˜è¦å‰‡åº« (Step 4-5)
rule_base = build_rule_base(filtered_rules_df)

# æ¨¡æ“¬æª”æ¡ˆä¸‹è¼‰
with open("rule_base.json", "w", encoding="utf-8") as f:
   json.dump(rule_base, f, ensure_ascii=False, indent=2)
files.download("rule_base.json")
print(f"ç¸½å…±å»ºç«‹ {len(rule_base)} æ¢é«˜å“è³ªè¦å‰‡ã€‚")


# --- C. Hybrid RAG è¦å‰‡æª¢ç´¢ (Step 5) ---

# æ¸¬è©¦æ¡ˆä¾‹ï¼šå­¸ç”ŸéŒ¯èª¤çš„èƒ½åŠ›æ¨™ç±¤
student_errors = ["å¥æ®µ_é–±è®€", "ç¯‡ç« _é–±è®€", "å­—è©_æ‡‰ç”¨"]

# æª¢ç´¢æœ€ç›¸é—œè¦å‰‡ (top_k=7)
top_rules = retrieve_rules(
    student_abilities=student_errors,
    rule_base=rule_base,
    top_k=7
)
print(f"\næª¢ç´¢åˆ° {len(top_rules)} æ¢èˆ‡å­¸ç”ŸéŒ¯èª¤æœ€ç›¸é—œçš„è¦å‰‡ã€‚")


# --- D. å ±å‘Šç”Ÿæˆå‰ç½®æ•¸æ“š (Step 9 é‚è¼¯) ---

# æ¨¡æ“¬ Top N éŒ¯èª¤ç‡ DataFrame
data_topN = {
    'èƒ½åŠ›æ¨™ç±¤': ['è©å½™_è¾¨æ', 'å¥å­_çµæ§‹', 'ä¿®è¾­_æ‡‰ç”¨', 'æ–‡æœ¬_ä¸»æ—¨', 'å¥æ®µ_é–±è®€', 'å­—è©_æ‡‰ç”¨', 'ç¯‡ç« _é–±è®€', 'èªæ³•_åˆ¤æ–·', 'æ–‡å­¸_å¸¸è­˜', 'é‚è¼¯_æ¨è«–'],
    'éŒ¯èª¤ç‡': [0.85, 0.79, 0.72, 0.68, 0.65, 0.61, 0.58, 0.55, 0.50, 0.48]
}
top_ability_errors = pd.DataFrame(data_topN)
topN = top_ability_errors.head(10).rename(columns={'éŒ¯èª¤ç‡': 'éŒ¯èª¤ç‡'})


# --- E. è¼¸å‡º Markdown å ±å‘Š (Step 7/8 æ•´åˆ) ---

packet = render_markdown_packet(
    student_errors, 
    top_rules, 
    topN, 
    n_per_ability=2
)

# è¼¸å‡ºçµæœ
print("\n--- [generated_packet.md å…§å®¹æ¨¡æ“¬é–‹å§‹] ---")
print(packet)
print("--- [generated_packet.md å…§å®¹æ¨¡æ“¬çµæŸ] ---")

# æ¨¡æ“¬æª”æ¡ˆä¸‹è¼‰
with open("generated_packet.md","w",encoding="utf-8") as f:
    f.write(packet)
files.download("generated_packet.md")